{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in: Lin, M.-S., et al.: Malicious URL filtering- a big data application. IEEE Interna-\n",
    "tional Conference on Big Data (2013) we are going to extract features from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Lexical features extraction: split URLs into components, apply a sliding window to the domain, and use a bag-of-words model to describe each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicalFE(url): #lexical feature extraction fn - takes in a URL\n",
    "    # Ensure the URL has a scheme for proper parsing, crucial for IPv6 addr\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url  # prepend with default scheme\n",
    "    \n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        path = parsed_url.path\n",
    "        query = parsed_url.query\n",
    "        domain = domain.replace('www.', '')\n",
    "        features = {\n",
    "            'domain': domain,\n",
    "            'domain_length': len(domain),\n",
    "            'path_length': len(path),\n",
    "            'query_length': len(query),\n",
    "            'num_path_components': len(path.split('/')) - 1,  # Subtracting 1 because the leading '/' results in an empty string at the start\n",
    "            'num_query_components': len(query.split('&')) if query else 0,  # Only count if there's a query\n",
    "        }\n",
    "        features['has_digits_in_domain'] = any(char.isdigit() for char in domain)\n",
    "        return features\n",
    "    except ValueError as e: #handle errors TODO: more here\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: descriptive features extraction - this function will further split the path component, remove common prefixes and TLDs, and calc stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptiveFE(url): #descriptive feature extraction fn - takes in a URL\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc.replace('www.', '')  # Remove common prefix\n",
    "    path = parsed_url.path\n",
    "    query = parsed_url.query\n",
    "    path_components = path.split('/') # further split the path\n",
    "    filename = path_components[-1] if '.' in path_components[-1] else None\n",
    "    file_extension = filename.split('.')[-1] if filename else None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    features = {\n",
    "        'domain_length': len(domain),\n",
    "        'path_length': len(path),\n",
    "        'query_length': len(query),\n",
    "        'num_path_components': len(path_components),\n",
    "        'filename': filename,\n",
    "        'file_extension': file_extension,\n",
    "        'is_ip_address': bool(re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', domain)),\n",
    "        'executable_extension': file_extension in ['exe', 'bin', 'bat']\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: integrate and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.example.org/bin.exe?arg=value\n",
      "Lexical Features: {'domain': 'example.org', 'domain_length': 11, 'path_length': 8, 'query_length': 9, 'num_path_components': 1, 'num_query_components': 1, 'has_digits_in_domain': False}\n",
      "Descriptive Features: {'domain_length': 11, 'path_length': 8, 'query_length': 9, 'num_path_components': 2, 'filename': 'bin.exe', 'file_extension': 'exe', 'is_ip_address': False, 'executable_extension': True}\n",
      "\n",
      "\n",
      "URL: http://blog.example.com:443/executable.exe?arg=test123\n",
      "Lexical Features: {'domain': 'blog.example.com:443', 'domain_length': 20, 'path_length': 15, 'query_length': 11, 'num_path_components': 1, 'num_query_components': 1, 'has_digits_in_domain': True}\n",
      "Descriptive Features: {'domain_length': 20, 'path_length': 15, 'query_length': 11, 'num_path_components': 2, 'filename': 'executable.exe', 'file_extension': 'exe', 'is_ip_address': False, 'executable_extension': True}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CAUTION: neutered phishing URL: urls = ['{http colon slash slash}clt1658125{dot}benchurl{dot}com']\n",
    "urls = ['https://www.example.org/bin.exe?arg=value', 'http://blog.example.com:443/executable.exe?arg=test123']\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"URL: {url}\")\n",
    "    lexFeats = lexicalFE(url) # lexical features\n",
    "    descFeats = descriptiveFE(url) # descriptive features\n",
    "    print(\"Lexical Features:\", lexFeats)\n",
    "    print(\"Descriptive Features:\", descFeats)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('malicious_phish-kaggle-thishusseinali.csv', names=['URL', 'Classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL http://RybjUx\u0018Ùãl5»7ÆE%Ý\u0014Ôk+h\u001f\u000f|U\u001a\u0007+ýk©ìÉ½Æq]âF·õÁ¢w)ëA·ç\u000f°{t*m!¦2\u0003: Invalid IPv6 URL\n",
      "Error processing URL http://Æe\u001eF§÷%\u0011¶\u001c¿Õ½9¿b@Ö¸ÚZE¤ÒC¢ÄÅª2åç-]W³fU¤\u001eJgkz.ø¿nJçå\u0014æuøD%@ðû\u000fÇùM¹u\u000bË: Invalid IPv6 URL\n",
      "Error processing URL http://Ó6¸RTÃ\u0006u\u000f~æ\fÙ\u001dg0>÷mÖi\u0012Ó=;XZ\\%êýÜ\u0005Éfn&\\°%7õÉ\"ieÖ\u000f\u00161ÄÁêFÐò\u0017<\u0018$cï6t[0ò2\"/Æa^2â\u001fpù/ýãÇ$E¬R«È²ú\u0006[Ì¶p\u001f¥qÒ°i°^ò[»³»]±9êdÓS¿Ë]ùþ5j¿·ªocÂplà7Ê\u000bÏJ§¢#3\u001bðDCD\u0012\u0002õ²çÇ\u0017GÝ.Vò=¿QB§Ä'`ÊáZÉê Ô\u0010îÆm®ÍÝQÓ(z;¹\u001bÁ\u0003ê¬âyt\u001bÖ\u0007Ù®ëNP²ÜEQ: Invalid IPv6 URL\n",
      "Error processing URL http://µÔA¨!ÝÛ=]º£¦Pôwr7\u001f2\u0007-ÕY5Äòè7¬\u000f-³]×)&¡\u001fe¸\u001c¢\u0014À6R\u0018D­NvY¨Ð«Ñ3Â¸%Qñ+Û\u0010È¸\u0003\u0012$¶gz{þ: Invalid IPv6 URL\n",
      "Error processing URL http://¨RÊÃûaCóÞit×ßÂe-DÖØ+9YèÌçÏ¯·\u0004\"0£ÙÕ.0ößF«7¹NR\u001c\u0004Ù{ccÉÄãéçx[Ä6a\u001a5Ñ³LÖíÜÉÀ£Òma¥yRX\u0003*0ÅÝ7×ÊÁÌ\u0005\u0005o«Õs¶0kdèÑ&\u001cÄ\u0010\"Ï¨mZ'àDM×ñ\u0001XÚÒK\"päî±h¬cAÊeK@4r\"^'ÓFþ1*Ë\u001dË PÞô;õ$úàÑ@þ=êWÑ\"Ãhñ\u0005\u0018®ç^\u0018\u0011«Ýó^ç\u001a\u001fRúUJ\u0014.<6C\u0019y\u001aÜFØrÿV2ôæý\u0003Zãii\u0016I\u000eb;\u0013\u0016¨Ëµu^ÍVy)­è»âýº\u0001+SÖ\u001e\u0017á\u0010\u0003Ãì?\u001få6åÔ/: Invalid IPv6 URL\n",
      "Error processing URL http://1]Î¼0#W»æ½Î4>¥õ\u001cª(\\xl3(ò5?¹(°åþ¬eéÍû\u0012\u0006µÆÒÒ-&\u001d\u0014Äv&-Q/9jê½2­ òS;[ÑwÅût\u0002W?(§3¬</Â!*\u0007Ø~?ÊmË¨^XV¹µÂ¦\u00183¨|[÷4\u0010fÈë<\tô·»n³HéÜúÂÒá/Wîà.K3q4:å)¿®I\u0013K.°x±&\u000fR6¹àÄ\u0001#\u001f|9³¢Üùñ\u0014\u0019~3: Invalid IPv6 URL\n",
      "Error processing URL http://k¥¤ZM$:)ìLZ£.^rÕÕ{6eZAä¦v·¢ïöè\u0004®~QNgXx_BT\u000f\u000f°üÛP¼°­9sk%L0gPø·îh ÍÖx\u0013\u0003éovÝf-3Ó¹õ¥Â^¯ÝÀ\\fâhuË\u0002S\\&]`ñc\u001cL\u001e¤m\u000f!sÁ\u0002F>øsgE¹\u0013\u000f)ó)â(2üf\u0015ã).!÷\u000fÿÞ<rDZÅ*¼/ e¼Ëh\u0001úW\u001d+\u0006\u0005ï%»»;µÛ\u0013¦M5;ù\\¸¥ßãV«û°z¦ö9Ì'\u0006Î\u0003| ¼ôªz\"8#1¿\u0004D4A /[.|qt}Òåo#Ûõ\u0014*|Bv\f®\u0018U¬¢: Invalid IPv6 URL\n",
      "Error processing URL http://A\u000eìfÙêìÝÕ\u001d2£»¸ü\u00199nªÉ®'A½`ymî¹èDéIû-ÄXå\u0010¦\u0001²Â%js²ÍD^^\u0015Á*^ÜD-À\u0018%³;Óßñg\\8+±Wnn@¾IßBëC[³3S7M(úJzª6¥\u0018HmÒ)fæ(\u0001þ\u0017i\u001e\u0013¡d \u0016ùp: Invalid IPv6 URL\n",
      "Error processing URL http://½\u0013<+U\u001f\u0012½¹1\f[1\"Kfw§¿âÎ¸pWb®ÜÜ\u0007|¾Ö\u0018·ff¦o6QõFõTmK&0Û¤RÍÌP\u0003ü.Ò<'BÉ@-ó\u001aàYN\u0001\\¦~7J¡*Èú=ÙU ^>R~@O·: Invalid IPv6 URL\n",
      "Error processing URL http://¯\u0004=ÓÛ±\u0013i-\u0011Ð\u0018]: Invalid IPv6 URL\n",
      "Error processing URL http://=Rã\u000fmôj³{è!ÀM¶6<N>w¼Cf\u0011£]\u001b4ÍnÝÌ: Invalid IPv6 URL\n",
      "Error processing URL http://9Ý&Ö\u001aW%9¢£\f]Ó}|¨<\u001cÀPVú½W;hÓ\u0006¹\u0006N\u0012h1ÌA\u0002: Invalid IPv6 URL\n",
      "Error processing URL http://òóÒ©èàÆü\u0011\u0010Ñ\u0004½l¹ñÅÑ÷K¼oÔP©ñ\tØ2¸aV²z5\u0004;±g«³wTtÊ\u001eîFãðf@åº«&¸]\u0006õå¶kºç¡\u0013¥$Ô#ì·\u0011cB\u0006Ê©ÎË5\u0011z\u0015z&xW\u0004ª>%{rb\u0019\u0010·: Invalid IPv6 URL\n",
      "URL                                   mp3raid.com/music/krizz_kaliko.html\n",
      "Classification                                                     benign\n",
      "Lexical_Features        {'domain': 'mp3raid.com', 'domain_length': 11,...\n",
      "Descriptive_Features    {'domain_length': 0, 'path_length': 35, 'query...\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply lexical and descriptive feature extraction\n",
    "df['Lexical_Features'] = df['URL'].apply(lambda x: lexicalFE(x))\n",
    "df['Descriptive_Features'] = df['URL'].apply(lambda x: descriptiveFE(x))\n",
    "\n",
    "testIndex = 2\n",
    "# Print the first row as an example (you can change the index to print another row)\n",
    "print(df.iloc[testIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Features for row 2:\n",
      "{\n",
      "    \"domain\": \"mp3raid.com\",\n",
      "    \"domain_length\": 11,\n",
      "    \"path_length\": 24,\n",
      "    \"query_length\": 0,\n",
      "    \"num_path_components\": 2,\n",
      "    \"num_query_components\": 0,\n",
      "    \"has_digits_in_domain\": true\n",
      "}\n",
      "\n",
      "Descriptive Features for row 2:\n",
      "{\n",
      "    \"domain_length\": 0,\n",
      "    \"path_length\": 35,\n",
      "    \"query_length\": 0,\n",
      "    \"num_path_components\": 3,\n",
      "    \"filename\": \"krizz_kaliko.html\",\n",
      "    \"file_extension\": \"html\",\n",
      "    \"is_ip_address\": false,\n",
      "    \"executable_extension\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the row index you're interested in\n",
    "row_index = testIndex  # For example, to print the second row\n",
    "\n",
    "# Convert the dictionary to a JSON string for pretty printing\n",
    "lexical_features_str = json.dumps(df.at[row_index, 'Lexical_Features'], indent=4)\n",
    "descriptive_features_str = json.dumps(df.at[row_index, 'Descriptive_Features'], indent=4)\n",
    "\n",
    "# Print the features\n",
    "print(f\"Lexical Features for row {row_index}:\\n{lexical_features_str}\\n\")\n",
    "print(f\"Descriptive Features for row {row_index}:\\n{descriptive_features_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to normalize the features and concatenate them with the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate empty dataframe\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# process in chunk size defined in previous cell\n",
    "for start in range(0, df.shape[0], chunk_size):\n",
    "    end = min(start + chunk_size, df.shape[0])\n",
    "    df_chunk = df.iloc[start:end].copy()\n",
    "    df_chunk.reset_index(drop=True, inplace=True)\n",
    "    # normalize lexical and descriptive features\n",
    "    lexFeatsDF = pd.json_normalize(df_chunk['Lexical_Features'])\n",
    "    lexFeatsDF.columns = ['Lexical_' + str(col) for col in lexFeatsDF.columns]\n",
    "    descFeatsDF = pd.json_normalize(df_chunk['Descriptive_Features'])\n",
    "    descFeatsDF.columns = ['Descriptive_' + str(col) for col in descFeatsDF.columns]\n",
    "    df_chunk = pd.concat([df_chunk, lexFeatsDF, descFeatsDF], axis=1) #concat normalized feats with chunk\n",
    "    df_final = pd.concat([df_final, df_chunk], axis=0, ignore_index=True) #direct append to final df\n",
    "\n",
    "# drop unnecessary column names\n",
    "df_final.drop(['Lexical_Features', 'Descriptive_Features'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class mapping: {'benign': 0, 'defacement': 1, 'malware': 2, 'phishing': 3}\n"
     ]
    }
   ],
   "source": [
    "df_final = df_final.drop(1) # drop row 1\n",
    "df_final = df_final.drop(['URL'], axis=1)\n",
    "df_final_columns = df_final.columns.tolist()\n",
    "with open('model_columns.txt', 'w') as f:\n",
    "    f.write('\\n'.join(df_final_columns))\n",
    "df_final = df_final.dropna() #drop rows with missing values\n",
    "catCols = df_final.select_dtypes(include=['object', 'category']).columns\n",
    "# convert categoricals\n",
    "for col in catCols:\n",
    "    # Skip the target column 'Classification'\n",
    "    if col == 'Classification':\n",
    "        continue\n",
    "    le = LabelEncoder()\n",
    "    df_final[col] = le.fit_transform(df_final[col])\n",
    "    # save the encoder\n",
    "    joblib.dump(le, 'categorical_feature_encoder.joblib')\n",
    "\n",
    "# split dataframe into features and target\n",
    "X = df_final.drop('Classification', axis=1)\n",
    "y = df_final['Classification']\n",
    "\n",
    "# Convert 'Classification' to numerical values if it's categorical\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a decision tree classifier\n",
    "dTree = DecisionTreeClassifier(random_state=42)\n",
    "dTree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"class mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree model: 0.94\n"
     ]
    }
   ],
   "source": [
    "# predict test set results\n",
    "y_pred = dTree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the Decision Tree model: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 'benign': 0, 'defacement': 1, 'malware': 2, 'phishing': 3\n",
      "Class of URL: 0\n"
     ]
    }
   ],
   "source": [
    "# encoder saved from before - training phase\n",
    "encoder = joblib.load('categorical_feature_encoder.joblib')\n",
    "\n",
    "def predict_url_classification(url, dTree, df_final_columns):\n",
    "    # Extract features\n",
    "    lexical_features = lexicalFE(url)\n",
    "    descriptive_features = descriptiveFE(url)\n",
    "    \n",
    "    # Combine features\n",
    "    all_features = {**lexical_features, **descriptive_features}\n",
    "    \n",
    "    # Create a DataFrame for the features\n",
    "    features_df = pd.DataFrame([all_features])\n",
    "    \n",
    "    # Ensure the DataFrame matches the training data structure\n",
    "    # Add missing columns with default values\n",
    "    for col in df_final_columns:\n",
    "        if col not in features_df.columns:\n",
    "            features_df[col] = 0  # Or another appropriate default value\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    features_df = features_df[df_final_columns]\n",
    "    \n",
    "    # Drop columns that are not features (e.g., 'URL', 'Classification' if they were included)\n",
    "    features_to_drop = ['URL', 'Classification']  # Adjust based on your actual data\n",
    "    features_df = features_df.drop(columns=[col for col in features_to_drop if col in features_df.columns], errors='ignore')\n",
    "    \n",
    "    # Predict the classification\n",
    "    prediction = dTree.predict(features_df)\n",
    "    \n",
    "    return prediction[0]  # Assuming binary classification for simplicity\n",
    "\n",
    "# Use with caution if you are pasting in real malicious domains\n",
    "# CAUTION # url = \"prefix of URL\"+ \"c/l?u=10C78AC0&e=17AD89B&c=194D0D&t=0&email=WqS0CM9o%2BpbtiwumbI%2Fj2w%3D%3D&seq=1\"\n",
    "url = 'https://www.example.org/bin.exe?arg=value'\n",
    "classification = predict_url_classification(url, dTree, df_final_columns)\n",
    "print(f\"Classes: 'benign': 0, 'defacement': 1, 'malware': 2, 'phishing': 3\")\n",
    "print(f\"Class of URL: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a support vector classifier (SVC) using dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skl2onnx\n",
      "  Downloading skl2onnx-1.16.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting onnx>=1.2.1 (from skl2onnx)\n",
      "  Downloading onnx-1.15.0-cp310-cp310-macosx_10_12_universal2.whl.metadata (15 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from skl2onnx) (1.3.0)\n",
      "Collecting onnxconverter-common>=1.7.0 (from skl2onnx)\n",
      "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from onnx>=1.2.1->skl2onnx) (1.26.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from onnx>=1.2.1->skl2onnx) (4.23.4)\n",
      "Requirement already satisfied: packaging in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from onnxconverter-common>=1.7.0->skl2onnx) (23.2)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.2.1->skl2onnx)\n",
      "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from scikit-learn>=0.19->skl2onnx) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from scikit-learn>=0.19->skl2onnx) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from scikit-learn>=0.19->skl2onnx) (2.2.0)\n",
      "Downloading skl2onnx-1.16.0-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.5/298.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.15.0-cp310-cp310-macosx_10_12_universal2.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, onnx, onnxconverter-common, skl2onnx\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed onnx-1.15.0 onnxconverter-common-1.14.0 protobuf-3.20.2 skl2onnx-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install skl2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from joblib import Parallel, delayed\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# svc_model = SVC(random_state=42, verbose=True)\n",
    "# svc_model.fit(X_train, y_train)\n",
    "# 25m 0.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVC model: 0.60\n"
     ]
    }
   ],
   "source": [
    "# y_pred_svc = svc_model.predict(X_test)\n",
    "# accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "# print(f\"Accuracy of the SVC model: {accuracy_svc:.2f}\")\n",
    "# 6m 17.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svc_model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(svc_model, 'svc_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 15\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.shape[1]\n",
    "print(f\"Number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model\n",
    "model = joblib.load('svc_model.joblib')\n",
    "\n",
    "# Define initial types for the model conversion\n",
    "initial_type = [('float_input', FloatTensorType([None, 15]))]  # Example for a model with 4 features\n",
    "\n",
    "# Convert the model\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "\n",
    "# Save the model\n",
    "with open(\"model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 33.33 MB\n"
     ]
    }
   ],
   "source": [
    "# Determine model size\n",
    "import os\n",
    "model_path = 'svc_model.joblib'\n",
    "model_size_bytes = os.path.getsize(model_path)\n",
    "model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.17.1-cp310-cp310-macosx_11_0_universal2.whl.metadata (4.2 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from onnxruntime) (1.26.3)\n",
      "Requirement already satisfied: packaging in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: protobuf in /Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages (from onnxruntime) (3.20.2)\n",
      "Collecting sympy (from onnxruntime)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->onnxruntime)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading onnxruntime-1.17.1-cp310-cp310-macosx_11_0_universal2.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 mpmath-1.3.0 onnxruntime-1.17.1 sympy-1.12\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "sess = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Prepare your input data in the correct format (numpy array)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "input_data = np.random.randn(10, num_features).astype(np.float32)  # Example input\n",
    "\n",
    "# Run the model\n",
    "predictions = sess.run([output_name], {input_name: input_data})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-class2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
