{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of the code to process the dataset can be found in `1-pup.ipynb`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Lexical features extraction: split URLs into components, apply a sliding window to the domain, and use a bag-of-words model to describe each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicalFE(url): #lexical feature extraction fn - takes in a URL\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url  # prepend with default scheme\n",
    "    \n",
    "    try:\n",
    "        parsedURL = urlparse(url)\n",
    "        domain = parsedURL.netloc\n",
    "        path = parsedURL.path\n",
    "        query = parsedURL.query\n",
    "        domainNoPrefix = domain.replace('www.', '')\n",
    "        features = {\n",
    "            'domain': domain,\n",
    "            'domainNoPrefix': domainNoPrefix,\n",
    "            'domainLength': len(domain),\n",
    "            'pathLength': len(path),\n",
    "            'queryLength': len(query),\n",
    "            'numPathComponents': len(path.split('/')) - 1,  # Subtracting 1 because the leading '/' results in an empty string at the start\n",
    "            'numQueryComponents': len(query.split('&')) if query else 0,  # Only count if there's a query\n",
    "            'hasDigitsInDomain': any(char.isdigit() for char in domain),\n",
    "            'hasDigitsInPath': any(char.isdigit() for char in path),\n",
    "            'hasDigitsInQuery': any(char.isdigit() for char in query)\n",
    "        }\n",
    "        return features\n",
    "    except ValueError as e: #handle errors TODO: more here\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: descriptive features extraction - this function will further split the path component, remove common prefixes and TLDs, and calc stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptiveFE(url): #descriptive feature extraction fn - takes in a URL\n",
    "    parsedURL = urlparse(url)\n",
    "    domain = parsedURL.netloc.replace('www.', '')  # Remove common prefix\n",
    "    path = parsedURL.path\n",
    "    query = parsedURL.query\n",
    "    path_components = path.split('/') # further split the path\n",
    "    filename = path_components[-1] if '.' in path_components[-1] else None\n",
    "    fileBool = 1 if filename else 0\n",
    "    file_extension = filename.split('.')[-1] if filename else None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    features = {\n",
    "        'domainLength': len(domain),\n",
    "        'pathLength': len(path),\n",
    "        'queryLength': len(query),\n",
    "        'numPathComponents': len(path_components),\n",
    "        'filename': filename,\n",
    "        'fileNamePresent': fileBool,\n",
    "        'fileExtension': file_extension,\n",
    "        'isIpAdress': bool(re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', domain)),\n",
    "        'fileExecutable': file_extension in ['exe', 'bin', 'bat']\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/conglom-labeled.csv', names=['URL', 'Classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply lexical and descriptive feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL http://RybjUx\u0018Ùãl5»7ÆE%Ý\u0014Ôk+h\u001f\u000f|U\u001a\u0007+ýk©ìÉ½Æq]âF·õÁ¢w)ëA·ç\u000f°{t*m!¦2\u0003: Invalid IPv6 URL\n",
      "Error processing URL http://Æe\u001eF§÷%\u0011¶\u001c¿Õ½9¿b@Ö¸ÚZE¤ÒC¢ÄÅª2åç-]W³fU¤\u001eJgkz.ø¿nJçå\u0014æuøD%@ðû\u000fÇùM¹u\u000bË: Invalid IPv6 URL\n",
      "Error processing URL http://Ó6¸RTÃ\u0006u\u000f~æ\fÙ\u001dg0>÷mÖi\u0012Ó=;XZ\\%êýÜ\u0005Éfn&\\°%7õÉ\"ieÖ\u000f\u00161ÄÁêFÐò\u0017<\u0018$cï6t[0ò2\"/Æa^2â\u001fpù/ýãÇ$E¬R«È²ú\u0006[Ì¶p\u001f¥qÒ°i°^ò[»³»]±9êdÓS¿Ë]ùþ5j¿·ªocÂplà7Ê\u000bÏJ§¢#3\u001bðDCD\u0012\u0002õ²çÇ\u0017GÝ.Vò=¿QB§Ä'`ÊáZÉê Ô\u0010îÆm®ÍÝQÓ(z;¹\u001bÁ\u0003ê¬âyt\u001bÖ\u0007Ù®ëNP²ÜEQ: Invalid IPv6 URL\n",
      "Error processing URL http://µÔA¨!ÝÛ=]º£¦Pôwr7\u001f2\u0007-ÕY5Äòè7¬\u000f-³]×)&¡\u001fe¸\u001c¢\u0014À6R\u0018D­NvY¨Ð«Ñ3Â¸%Qñ+Û\u0010È¸\u0003\u0012$¶gz{þ: Invalid IPv6 URL\n",
      "Error processing URL http://¨RÊÃûaCóÞit×ßÂe-DÖØ+9YèÌçÏ¯·\u0004\"0£ÙÕ.0ößF«7¹NR\u001c\u0004Ù{ccÉÄãéçx[Ä6a\u001a5Ñ³LÖíÜÉÀ£Òma¥yRX\u0003*0ÅÝ7×ÊÁÌ\u0005\u0005o«Õs¶0kdèÑ&\u001cÄ\u0010\"Ï¨mZ'àDM×ñ\u0001XÚÒK\"päî±h¬cAÊeK@4r\"^'ÓFþ1*Ë\u001dË PÞô;õ$úàÑ@þ=êWÑ\"Ãhñ\u0005\u0018®ç^\u0018\u0011«Ýó^ç\u001a\u001fRúUJ\u0014.<6C\u0019y\u001aÜFØrÿV2ôæý\u0003Zãii\u0016I\u000eb;\u0013\u0016¨Ëµu^ÍVy)­è»âýº\u0001+SÖ\u001e\u0017á\u0010\u0003Ãì?\u001få6åÔ/: Invalid IPv6 URL\n",
      "Error processing URL http://1]Î¼0#W»æ½Î4>¥õ\u001cª(\\xl3(ò5?¹(°åþ¬eéÍû\u0012\u0006µÆÒÒ-&\u001d\u0014Äv&-Q/9jê½2­ òS;[ÑwÅût\u0002W?(§3¬</Â!*\u0007Ø~?ÊmË¨^XV¹µÂ¦\u00183¨|[÷4\u0010fÈë<\tô·»n³HéÜúÂÒá/Wîà.K3q4:å)¿®I\u0013K.°x±&\u000fR6¹àÄ\u0001#\u001f|9³¢Üùñ\u0014\u0019~3: Invalid IPv6 URL\n",
      "Error processing URL http://k¥¤ZM$:)ìLZ£.^rÕÕ{6eZAä¦v·¢ïöè\u0004®~QNgXx_BT\u000f\u000f°üÛP¼°­9sk%L0gPø·îh ÍÖx\u0013\u0003éovÝf-3Ó¹õ¥Â^¯ÝÀ\\fâhuË\u0002S\\&]`ñc\u001cL\u001e¤m\u000f!sÁ\u0002F>øsgE¹\u0013\u000f)ó)â(2üf\u0015ã).!÷\u000fÿÞ<rDZÅ*¼/ e¼Ëh\u0001úW\u001d+\u0006\u0005ï%»»;µÛ\u0013¦M5;ù\\¸¥ßãV«û°z¦ö9Ì'\u0006Î\u0003| ¼ôªz\"8#1¿\u0004D4A /[.|qt}Òåo#Ûõ\u0014*|Bv\f®\u0018U¬¢: Invalid IPv6 URL\n",
      "Error processing URL http://A\u000eìfÙêìÝÕ\u001d2£»¸ü\u00199nªÉ®'A½`ymî¹èDéIû-ÄXå\u0010¦\u0001²Â%js²ÍD^^\u0015Á*^ÜD-À\u0018%³;Óßñg\\8+±Wnn@¾IßBëC[³3S7M(úJzª6¥\u0018HmÒ)fæ(\u0001þ\u0017i\u001e\u0013¡d \u0016ùp: Invalid IPv6 URL\n",
      "Error processing URL http://½\u0013<+U\u001f\u0012½¹1\f[1\"Kfw§¿âÎ¸pWb®ÜÜ\u0007|¾Ö\u0018·ff¦o6QõFõTmK&0Û¤RÍÌP\u0003ü.Ò<'BÉ@-ó\u001aàYN\u0001\\¦~7J¡*Èú=ÙU ^>R~@O·: Invalid IPv6 URL\n",
      "Error processing URL http://¯\u0004=ÓÛ±\u0013i-\u0011Ð\u0018]: Invalid IPv6 URL\n",
      "Error processing URL http://=Rã\u000fmôj³{è!ÀM¶6<N>w¼Cf\u0011£]\u001b4ÍnÝÌ: Invalid IPv6 URL\n",
      "Error processing URL http://9Ý&Ö\u001aW%9¢£\f]Ó}|¨<\u001cÀPVú½W;hÓ\u0006¹\u0006N\u0012h1ÌA\u0002: Invalid IPv6 URL\n",
      "Error processing URL http://òóÒ©èàÆü\u0011\u0010Ñ\u0004½l¹ñÅÑ÷K¼oÔP©ñ\tØ2¸aV²z5\u0004;±g«³wTtÊ\u001eîFãðf@åº«&¸]\u0006õå¶kºç¡\u0013¥$Ô#ì·\u0011cB\u0006Ê©ÎË5\u0011z\u0015z&xW\u0004ª>%{rb\u0019\u0010·: Invalid IPv6 URL\n",
      "URL                                                           0265331.com\n",
      "Classification                                                        ads\n",
      "Lexical_Features        {'domain': '0265331.com', 'domainNoPrefix': '0...\n",
      "Descriptive_Features    {'domainLength': 0, 'pathLength': 11, 'queryLe...\n",
      "Name: 12, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['Lexical_Features'] = df['URL'].apply(lambda x: lexicalFE(x))\n",
    "df['Descriptive_Features'] = df['URL'].apply(lambda x: descriptiveFE(x))\n",
    "testIndex = 12 # 12 this time\n",
    "print(df.iloc[testIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Features for row 12:\n",
      "{\n",
      "    \"domain\": \"0265331.com\",\n",
      "    \"domainNoPrefix\": \"0265331.com\",\n",
      "    \"domainLength\": 11,\n",
      "    \"pathLength\": 0,\n",
      "    \"queryLength\": 0,\n",
      "    \"numPathComponents\": 0,\n",
      "    \"numQueryComponents\": 0,\n",
      "    \"hasDigitsInDomain\": true,\n",
      "    \"hasDigitsInPath\": false,\n",
      "    \"hasDigitsInQuery\": false\n",
      "}\n",
      "\n",
      "Descriptive Features for row 12:\n",
      "{\n",
      "    \"domainLength\": 0,\n",
      "    \"pathLength\": 11,\n",
      "    \"queryLength\": 0,\n",
      "    \"numPathComponents\": 1,\n",
      "    \"filename\": \"0265331.com\",\n",
      "    \"fileNamePresent\": 1,\n",
      "    \"fileExtension\": \"com\",\n",
      "    \"isIpAdress\": false,\n",
      "    \"fileExecutable\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_index = testIndex \n",
    "lexical_features_str = json.dumps(df.at[row_index, 'Lexical_Features'], indent=4)\n",
    "descriptive_features_str = json.dumps(df.at[row_index, 'Descriptive_Features'], indent=4)\n",
    "print(f\"Lexical Features for row {row_index}:\\n{lexical_features_str}\\n\")\n",
    "print(f\"Descriptive Features for row {row_index}:\\n{descriptive_features_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to normalize the features and concatenate them with the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate empty dataframe\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# process in chunk size defined in previous cell\n",
    "for start in range(0, df.shape[0], chunk_size):\n",
    "    end = min(start + chunk_size, df.shape[0])\n",
    "    df_chunk = df.iloc[start:end].copy()\n",
    "    df_chunk.reset_index(drop=True, inplace=True)\n",
    "    # normalize lexical and descriptive features\n",
    "    lexFeatsDF = pd.json_normalize(df_chunk['Lexical_Features'])\n",
    "    lexFeatsDF.columns = ['Lexical_' + str(col) for col in lexFeatsDF.columns]\n",
    "    descFeatsDF = pd.json_normalize(df_chunk['Descriptive_Features'])\n",
    "    descFeatsDF.columns = ['Descriptive_' + str(col) for col in descFeatsDF.columns]\n",
    "    df_chunk = pd.concat([df_chunk, lexFeatsDF, descFeatsDF], axis=1) #concat normalized feats with chunk\n",
    "    df_final = pd.concat([df_final, df_chunk], axis=0, ignore_index=True) #direct append to final df\n",
    "\n",
    "# drop unnecessary column names\n",
    "df_final.drop(['Lexical_Features', 'Descriptive_Features'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(1) # drop row 1\n",
    "df_final = df_final.drop(['URL'], axis=1)\n",
    "df_final_columns = df_final.columns.tolist()\n",
    "with open('model_columns.txt', 'w') as f:\n",
    "    f.write('\\n'.join(df_final_columns))\n",
    "df_final = df_final.dropna() #drop rows with missing values\n",
    "catCols = df_final.select_dtypes(include=['object', 'category']).columns\n",
    "# convert categoricals\n",
    "for col in catCols:\n",
    "    # skip the target column 'Classification'\n",
    "    if col == 'Classification':\n",
    "        continue\n",
    "    le = LabelEncoder()\n",
    "    df_final[col] = le.fit_transform(df_final[col])\n",
    "    # save the encoder\n",
    "    joblib.dump(le, './models-checkpoints/categorical_feature_encoder.joblib')\n",
    "\n",
    "# split dataframe into features and target\n",
    "X = df_final.drop('Classification', axis=1)\n",
    "y = df_final['Classification']\n",
    "\n",
    "# convert 'Classification' to numerical vals\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "# split training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CNN = X_train # capturing this pre-scaling\n",
    "X_test_CNN = X_test\n",
    "y_train_CNN = y_train\n",
    "y_test_CNN = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Lexical_Features</th>\n",
       "      <th>Descriptive_Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>URL</td>\n",
       "      <td>type</td>\n",
       "      <td>{'domain': 'URL', 'domainNoPrefix': 'URL', 'do...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 3, 'queryLen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008d6ba2e.com</td>\n",
       "      <td>ads</td>\n",
       "      <td>{'domain': '0008d6ba2e.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024ad98dd.com</td>\n",
       "      <td>ads</td>\n",
       "      <td>{'domain': '0024ad98dd.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0083334e84.com</td>\n",
       "      <td>ads</td>\n",
       "      <td>{'domain': '0083334e84.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d3ed994e.com</td>\n",
       "      <td>ads</td>\n",
       "      <td>{'domain': '00d3ed994e.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              URL Classification  \\\n",
       "0             URL           type   \n",
       "1  0008d6ba2e.com            ads   \n",
       "2  0024ad98dd.com            ads   \n",
       "3  0083334e84.com            ads   \n",
       "4  00d3ed994e.com            ads   \n",
       "\n",
       "                                    Lexical_Features  \\\n",
       "0  {'domain': 'URL', 'domainNoPrefix': 'URL', 'do...   \n",
       "1  {'domain': '0008d6ba2e.com', 'domainNoPrefix':...   \n",
       "2  {'domain': '0024ad98dd.com', 'domainNoPrefix':...   \n",
       "3  {'domain': '0083334e84.com', 'domainNoPrefix':...   \n",
       "4  {'domain': '00d3ed994e.com', 'domainNoPrefix':...   \n",
       "\n",
       "                                Descriptive_Features  \n",
       "0  {'domainLength': 0, 'pathLength': 3, 'queryLen...  \n",
       "1  {'domainLength': 0, 'pathLength': 14, 'queryLe...  \n",
       "2  {'domainLength': 0, 'pathLength': 14, 'queryLe...  \n",
       "3  {'domainLength': 0, 'pathLength': 14, 'queryLe...  \n",
       "4  {'domainLength': 0, 'pathLength': 14, 'queryLe...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Lexical_Features</th>\n",
       "      <th>Descriptive_Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>URL</td>\n",
       "      <td>6</td>\n",
       "      <td>{'domain': 'URL', 'domainNoPrefix': 'URL', 'do...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 3, 'queryLen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008d6ba2e.com</td>\n",
       "      <td>0</td>\n",
       "      <td>{'domain': '0008d6ba2e.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024ad98dd.com</td>\n",
       "      <td>0</td>\n",
       "      <td>{'domain': '0024ad98dd.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0083334e84.com</td>\n",
       "      <td>0</td>\n",
       "      <td>{'domain': '0083334e84.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d3ed994e.com</td>\n",
       "      <td>0</td>\n",
       "      <td>{'domain': '00d3ed994e.com', 'domainNoPrefix':...</td>\n",
       "      <td>{'domainLength': 0, 'pathLength': 14, 'queryLe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              URL  Classification  \\\n",
       "0             URL               6   \n",
       "1  0008d6ba2e.com               0   \n",
       "2  0024ad98dd.com               0   \n",
       "3  0083334e84.com               0   \n",
       "4  00d3ed994e.com               0   \n",
       "\n",
       "                                    Lexical_Features  \\\n",
       "0  {'domain': 'URL', 'domainNoPrefix': 'URL', 'do...   \n",
       "1  {'domain': '0008d6ba2e.com', 'domainNoPrefix':...   \n",
       "2  {'domain': '0024ad98dd.com', 'domainNoPrefix':...   \n",
       "3  {'domain': '0083334e84.com', 'domainNoPrefix':...   \n",
       "4  {'domain': '00d3ed994e.com', 'domainNoPrefix':...   \n",
       "\n",
       "                                Descriptive_Features  \n",
       "0  {'domainLength': 0, 'pathLength': 3, 'queryLen...  \n",
       "1  {'domainLength': 0, 'pathLength': 14, 'queryLe...  \n",
       "2  {'domainLength': 0, 'pathLength': 14, 'queryLe...  \n",
       "3  {'domainLength': 0, 'pathLength': 14, 'queryLe...  \n",
       "4  {'domainLength': 0, 'pathLength': 14, 'queryLe...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Creating a new DataFrame\n",
    "df_new = df.copy()\n",
    "\n",
    "# Initializing the Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transforming the 'Classification' column to numerical\n",
    "df_new['Classification'] = label_encoder.fit_transform(df_new['Classification'])\n",
    "\n",
    "# Displaying the first few rows of the new DataFrame\n",
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# urls = list(df['URL'])\n",
    "# encoding = tokenizer(urls, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `encoding` is your BatchEncoding object from the tokenizer\n",
    "# with open('./models-checkpoints/bert-url-encodings.pkl', 'wb') as f:\n",
    "#     pickle.dump(encoding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models-checkpoints/bert-url-encodings.pkl', 'rb') as f:\n",
    "    encoding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "labels_list = df_new['Classification'].tolist()\n",
    "print(labels_list[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor = torch.tensor(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(encoding['input_ids'], encoding['attention_mask'], labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m      3\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n\u001b[0;32m----> 5\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_split\u001b[49m(dataset, [train_size, val_size])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the dataset into 90% training and 10% validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming `dataset` is your original dataset\n",
    "total_size = len(dataset)\n",
    "small_size = int(0.01 * total_size)  # 1% - For example, 0.1 for 10% of the dataset\n",
    "\n",
    "# Split your dataset into a smaller dataset and a remainder (which we won't use here)\n",
    "small_dataset, _ = random_split(dataset, [small_size, total_size - small_size])\n",
    "\n",
    "# Create a DataLoader for the smaller dataset\n",
    "small_train_loader = DataLoader(small_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "num_labels = 6\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanhalloran/miniconda3/envs/ml-class2/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 4  # Number of training epochs. BERT authors recommend 2, 3, or 4.\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST RUN w/ 1 epoch, 10% of dataset via small_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-class2/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:82\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     80\u001b[0m         clip_coef_clamped_device \u001b[38;5;241m=\u001b[39m clip_coef_clamped\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads:\n\u001b[0;32m---> 82\u001b[0m             \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using MPS.\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(\"mps\")\n",
    "\n",
    "# Use a smaller number of epochs for the test run\n",
    "epochs = 1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(small_train_loader):  # Use the smaller loader\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Test run took {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[232], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     27\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-class2/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-class2/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs): # TRAINING\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Run took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)            \n",
    "    print(f\"Average training loss: {avg_train_loss}\")\n",
    "\n",
    "    # EVALUATION\n",
    "    \n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    print(f\"Validation loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models-checkpoints/bert/bert-url-classifier-state.pth')\n",
    "torch.save(model, './models-checkpoints/bert/bert-url-classifier-full-model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./models-checkpoints/bert/bert-url-classifier-full-model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-class2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
