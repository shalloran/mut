{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in: Lin, M.-S., et al.: Malicious URL filtering- a big data application. IEEE Interna-\n",
    "tional Conference on Big Data (2013) we are going to extract features from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Lexical features extraction: split URLs into components, apply a sliding window to the domain, and use a bag-of-words model to describe each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lexical_features(url):\n",
    "    # Ensure the URL has a scheme for proper parsing, crucial for IPv6 addresses\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url  # Prepend with a default scheme if missing\n",
    "    \n",
    "    try:\n",
    "        # Split URL into components\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        path = parsed_url.path\n",
    "        query = parsed_url.query\n",
    "\n",
    "        # Remove common prefixes from domain\n",
    "        domain = domain.replace('www.', '')\n",
    "\n",
    "        # Calculate features\n",
    "        features = {\n",
    "            'domain': domain,\n",
    "            'domain_length': len(domain),\n",
    "            'path_length': len(path),\n",
    "            'query_length': len(query),\n",
    "            'num_path_components': len(path.split('/')) - 1,  # Subtracting 1 because the leading '/' results in an empty string at the start\n",
    "            'num_query_components': len(query.split('&')) if query else 0,  # Only count if there's a query\n",
    "        }\n",
    "\n",
    "        # Additional processing to identify specific features, e.g., presence of digits in domain\n",
    "        features['has_digits_in_domain'] = any(char.isdigit() for char in domain)\n",
    "        \n",
    "        return features\n",
    "    except ValueError as e:\n",
    "        # Handle specific errors, e.g., invalid IPv6 URL\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: descriptive features extraction - this function will further split the path component, remove common prefixes and TLDs, and calc stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptive_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc.replace('www.', '')  # Remove common prefix\n",
    "    path = parsed_url.path\n",
    "    query = parsed_url.query\n",
    "    \n",
    "    # Further split the path\n",
    "    path_components = path.split('/')\n",
    "    filename = path_components[-1] if '.' in path_components[-1] else None\n",
    "    file_extension = filename.split('.')[-1] if filename else None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    features = {\n",
    "        'domain_length': len(domain),\n",
    "        'path_length': len(path),\n",
    "        'query_length': len(query),\n",
    "        'num_path_components': len(path_components),\n",
    "        'filename': filename,\n",
    "        'file_extension': file_extension,\n",
    "        'is_ip_address': bool(re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', domain)),\n",
    "        'executable_extension': file_extension in ['exe', 'bin', 'bat']\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: integrate and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.example.org/bin.exe?arg=value\n",
      "Lexical Features: {'domain': 'example.org', 'domain_length': 11, 'path_length': 8, 'query_length': 9, 'num_path_components': 1, 'num_query_components': 1, 'has_digits_in_domain': False}\n",
      "Descriptive Features: {'domain_length': 11, 'path_length': 8, 'query_length': 9, 'num_path_components': 2, 'filename': 'bin.exe', 'file_extension': 'exe', 'is_ip_address': False, 'executable_extension': True}\n",
      "\n",
      "\n",
      "URL: http://blog.example.com:443/executable.exe?arg=test123\n",
      "Lexical Features: {'domain': 'blog.example.com:443', 'domain_length': 20, 'path_length': 15, 'query_length': 11, 'num_path_components': 1, 'num_query_components': 1, 'has_digits_in_domain': True}\n",
      "Descriptive Features: {'domain_length': 20, 'path_length': 15, 'query_length': 11, 'num_path_components': 2, 'filename': 'executable.exe', 'file_extension': 'exe', 'is_ip_address': False, 'executable_extension': True}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CAUTION: neutered phishing URL: urls = ['{http colon slash slash}clt1658125{dot}benchurl{dot}com']\n",
    "urls = ['https://www.example.org/bin.exe?arg=value', 'http://blog.example.com:443/executable.exe?arg=test123']\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"URL: {url}\")\n",
    "    lexical_features = extract_lexical_features(url)\n",
    "    descriptive_features = extract_descriptive_features(url)\n",
    "    print(\"Lexical Features:\", lexical_features)\n",
    "    print(\"Descriptive Features:\", descriptive_features)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('malicious_phish-kaggle-thishusseinali.csv', names=['URL', 'Classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL http://RybjUx\u0018Ùãl5»7ÆE%Ý\u0014Ôk+h\u001f\u000f|U\u001a\u0007+ýk©ìÉ½Æq]âF·õÁ¢w)ëA·ç\u000f°{t*m!¦2\u0003: Invalid IPv6 URL\n",
      "Error processing URL http://Æe\u001eF§÷%\u0011¶\u001c¿Õ½9¿b@Ö¸ÚZE¤ÒC¢ÄÅª2åç-]W³fU¤\u001eJgkz.ø¿nJçå\u0014æuøD%@ðû\u000fÇùM¹u\u000bË: Invalid IPv6 URL\n",
      "Error processing URL http://Ó6¸RTÃ\u0006u\u000f~æ\fÙ\u001dg0>÷mÖi\u0012Ó=;XZ\\%êýÜ\u0005Éfn&\\°%7õÉ\"ieÖ\u000f\u00161ÄÁêFÐò\u0017<\u0018$cï6t[0ò2\"/Æa^2â\u001fpù/ýãÇ$E¬R«È²ú\u0006[Ì¶p\u001f¥qÒ°i°^ò[»³»]±9êdÓS¿Ë]ùþ5j¿·ªocÂplà7Ê\u000bÏJ§¢#3\u001bðDCD\u0012\u0002õ²çÇ\u0017GÝ.Vò=¿QB§Ä'`ÊáZÉê Ô\u0010îÆm®ÍÝQÓ(z;¹\u001bÁ\u0003ê¬âyt\u001bÖ\u0007Ù®ëNP²ÜEQ: Invalid IPv6 URL\n",
      "Error processing URL http://µÔA¨!ÝÛ=]º£¦Pôwr7\u001f2\u0007-ÕY5Äòè7¬\u000f-³]×)&¡\u001fe¸\u001c¢\u0014À6R\u0018D­NvY¨Ð«Ñ3Â¸%Qñ+Û\u0010È¸\u0003\u0012$¶gz{þ: Invalid IPv6 URL\n",
      "Error processing URL http://¨RÊÃûaCóÞit×ßÂe-DÖØ+9YèÌçÏ¯·\u0004\"0£ÙÕ.0ößF«7¹NR\u001c\u0004Ù{ccÉÄãéçx[Ä6a\u001a5Ñ³LÖíÜÉÀ£Òma¥yRX\u0003*0ÅÝ7×ÊÁÌ\u0005\u0005o«Õs¶0kdèÑ&\u001cÄ\u0010\"Ï¨mZ'àDM×ñ\u0001XÚÒK\"päî±h¬cAÊeK@4r\"^'ÓFþ1*Ë\u001dË PÞô;õ$úàÑ@þ=êWÑ\"Ãhñ\u0005\u0018®ç^\u0018\u0011«Ýó^ç\u001a\u001fRúUJ\u0014.<6C\u0019y\u001aÜFØrÿV2ôæý\u0003Zãii\u0016I\u000eb;\u0013\u0016¨Ëµu^ÍVy)­è»âýº\u0001+SÖ\u001e\u0017á\u0010\u0003Ãì?\u001få6åÔ/: Invalid IPv6 URL\n",
      "Error processing URL http://1]Î¼0#W»æ½Î4>¥õ\u001cª(\\xl3(ò5?¹(°åþ¬eéÍû\u0012\u0006µÆÒÒ-&\u001d\u0014Äv&-Q/9jê½2­ òS;[ÑwÅût\u0002W?(§3¬</Â!*\u0007Ø~?ÊmË¨^XV¹µÂ¦\u00183¨|[÷4\u0010fÈë<\tô·»n³HéÜúÂÒá/Wîà.K3q4:å)¿®I\u0013K.°x±&\u000fR6¹àÄ\u0001#\u001f|9³¢Üùñ\u0014\u0019~3: Invalid IPv6 URL\n",
      "Error processing URL http://k¥¤ZM$:)ìLZ£.^rÕÕ{6eZAä¦v·¢ïöè\u0004®~QNgXx_BT\u000f\u000f°üÛP¼°­9sk%L0gPø·îh ÍÖx\u0013\u0003éovÝf-3Ó¹õ¥Â^¯ÝÀ\\fâhuË\u0002S\\&]`ñc\u001cL\u001e¤m\u000f!sÁ\u0002F>øsgE¹\u0013\u000f)ó)â(2üf\u0015ã).!÷\u000fÿÞ<rDZÅ*¼/ e¼Ëh\u0001úW\u001d+\u0006\u0005ï%»»;µÛ\u0013¦M5;ù\\¸¥ßãV«û°z¦ö9Ì'\u0006Î\u0003| ¼ôªz\"8#1¿\u0004D4A /[.|qt}Òåo#Ûõ\u0014*|Bv\f®\u0018U¬¢: Invalid IPv6 URL\n",
      "Error processing URL http://A\u000eìfÙêìÝÕ\u001d2£»¸ü\u00199nªÉ®'A½`ymî¹èDéIû-ÄXå\u0010¦\u0001²Â%js²ÍD^^\u0015Á*^ÜD-À\u0018%³;Óßñg\\8+±Wnn@¾IßBëC[³3S7M(úJzª6¥\u0018HmÒ)fæ(\u0001þ\u0017i\u001e\u0013¡d \u0016ùp: Invalid IPv6 URL\n",
      "Error processing URL http://½\u0013<+U\u001f\u0012½¹1\f[1\"Kfw§¿âÎ¸pWb®ÜÜ\u0007|¾Ö\u0018·ff¦o6QõFõTmK&0Û¤RÍÌP\u0003ü.Ò<'BÉ@-ó\u001aàYN\u0001\\¦~7J¡*Èú=ÙU ^>R~@O·: Invalid IPv6 URL\n",
      "Error processing URL http://¯\u0004=ÓÛ±\u0013i-\u0011Ð\u0018]: Invalid IPv6 URL\n",
      "Error processing URL http://=Rã\u000fmôj³{è!ÀM¶6<N>w¼Cf\u0011£]\u001b4ÍnÝÌ: Invalid IPv6 URL\n",
      "Error processing URL http://9Ý&Ö\u001aW%9¢£\f]Ó}|¨<\u001cÀPVú½W;hÓ\u0006¹\u0006N\u0012h1ÌA\u0002: Invalid IPv6 URL\n",
      "Error processing URL http://òóÒ©èàÆü\u0011\u0010Ñ\u0004½l¹ñÅÑ÷K¼oÔP©ñ\tØ2¸aV²z5\u0004;±g«³wTtÊ\u001eîFãðf@åº«&¸]\u0006õå¶kºç¡\u0013¥$Ô#ì·\u0011cB\u0006Ê©ÎË5\u0011z\u0015z&xW\u0004ª>%{rb\u0019\u0010·: Invalid IPv6 URL\n",
      "URL                                                      br-icloud.com.br\n",
      "Classification                                                   phishing\n",
      "Lexical_Features        {'domain': 'br-icloud.com.br', 'domain_length'...\n",
      "Descriptive_Features    {'domain_length': 0, 'path_length': 16, 'query...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply lexical feature extraction\n",
    "df['Lexical_Features'] = df['URL'].apply(lambda x: extract_lexical_features(x))\n",
    "\n",
    "# Apply descriptive feature extraction\n",
    "df['Descriptive_Features'] = df['URL'].apply(lambda x: extract_descriptive_features(x))\n",
    "\n",
    "# Print the first row as an example (you can change the index to print another row)\n",
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Features for row 1:\n",
      "{\n",
      "    \"domain\": \"br-icloud.com.br\",\n",
      "    \"domain_length\": 16,\n",
      "    \"path_length\": 0,\n",
      "    \"query_length\": 0,\n",
      "    \"num_path_components\": 0,\n",
      "    \"num_query_components\": 0,\n",
      "    \"has_digits_in_domain\": false\n",
      "}\n",
      "\n",
      "Descriptive Features for row 1:\n",
      "{\n",
      "    \"domain_length\": 0,\n",
      "    \"path_length\": 16,\n",
      "    \"query_length\": 0,\n",
      "    \"num_path_components\": 1,\n",
      "    \"filename\": \"br-icloud.com.br\",\n",
      "    \"file_extension\": \"br\",\n",
      "    \"is_ip_address\": false,\n",
      "    \"executable_extension\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the row index you're interested in\n",
    "row_index = 1  # For example, to print the second row\n",
    "\n",
    "# Convert the dictionary to a JSON string for pretty printing\n",
    "lexical_features_str = json.dumps(df.at[row_index, 'Lexical_Features'], indent=4)\n",
    "descriptive_features_str = json.dumps(df.at[row_index, 'Descriptive_Features'], indent=4)\n",
    "\n",
    "# Print the features\n",
    "print(f\"Lexical Features for row {row_index}:\\n{lexical_features_str}\\n\")\n",
    "print(f\"Descriptive Features for row {row_index}:\\n{descriptive_features_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5000  # Adjust based on your system's capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to hold the results\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# Process in chunks\n",
    "for start in range(0, df.shape[0], chunk_size):\n",
    "    end = min(start + chunk_size, df.shape[0])\n",
    "    df_chunk = df.iloc[start:end].copy()\n",
    "    df_chunk.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Normalize Lexical Features\n",
    "    lexical_features_df = pd.json_normalize(df_chunk['Lexical_Features'])\n",
    "    lexical_features_df.columns = ['Lexical_' + str(col) for col in lexical_features_df.columns]\n",
    "    \n",
    "    # Normalize Descriptive Features\n",
    "    descriptive_features_df = pd.json_normalize(df_chunk['Descriptive_Features'])\n",
    "    descriptive_features_df.columns = ['Descriptive_' + str(col) for col in descriptive_features_df.columns]\n",
    "    \n",
    "    # Concatenate normalized features with the chunk\n",
    "    df_chunk = pd.concat([df_chunk, lexical_features_df, descriptive_features_df], axis=1)\n",
    "    \n",
    "    # Directly append the processed chunk to the final DataFrame with ignore_index=True\n",
    "    df_final = pd.concat([df_final, df_chunk], axis=0) #ignore_index=True\n",
    "\n",
    "# Optionally, drop the original columns containing dictionaries\n",
    "df_final.drop(['Lexical_Features', 'Descriptive_Features'], axis=1, inplace=True)\n",
    "#print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for Classification: {'benign': 0, 'defacement': 1, 'malware': 2, 'phishing': 3}\n"
     ]
    }
   ],
   "source": [
    "# Drop row 1 (Note: Python uses 0-based indexing, so row 1 is the second row)\n",
    "df_final = df_final.drop(1)\n",
    "\n",
    "df_final = df_final.drop(['URL'], axis=1)\n",
    "        # Proceed with retraining your model here\n",
    "\n",
    "df_final_columns = df_final.columns.tolist()\n",
    "with open('model_columns.txt', 'w') as f:\n",
    "    f.write('\\n'.join(df_final_columns))\n",
    "\n",
    "# drop rows with missing values\n",
    "df_final = df_final.dropna()\n",
    "\n",
    "# Identify categorical features. This is a basic approach and might need adjustment based on your dataset.\n",
    "categorical_cols = df_final.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Convert categorical features to numerical values\n",
    "for col in categorical_cols:\n",
    "    # Skip the target column 'Classification'\n",
    "    if col == 'Classification':\n",
    "        continue\n",
    "    le = LabelEncoder()\n",
    "    df_final[col] = le.fit_transform(df_final[col])\n",
    "    # save the encoder\n",
    "    joblib.dump(le, 'categorical_feature_encoder.joblib')\n",
    "\n",
    "# Split the DataFrame into X (features) and y (target)\n",
    "X = df_final.drop('Classification', axis=1)\n",
    "y = df_final['Classification']\n",
    "\n",
    "# Convert 'Classification' to numerical values if it's categorical\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a decision tree classifier\n",
    "dTree = DecisionTreeClassifier(random_state=42)\n",
    "dTree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Mapping for Classification: {dict(zip(le.classes_, le.transform(le.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree model: 0.94\n"
     ]
    }
   ],
   "source": [
    "# predict test set results\n",
    "y_pred = dTree.predict(X_test)\n",
    "\n",
    "# use accuracy_score to test the model\n",
    "accuracy = accuracy_score(y_test, y_pred) # as in M5_Decision Trees-1.ipynb\n",
    "print(f\"Accuracy of the Decision Tree model: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 'benign': 0, 'defacement': 1, 'malware': 2, 'phishing': 3\n",
      "Class of URL: 2\n"
     ]
    }
   ],
   "source": [
    "# encoder saved from before - training phase\n",
    "encoder = joblib.load('categorical_feature_encoder.joblib')\n",
    "\n",
    "def predict_url_classification(url, dTree, df_final_columns):\n",
    "    # Extract features\n",
    "    lexical_features = extract_lexical_features(url)\n",
    "    descriptive_features = extract_descriptive_features(url)\n",
    "    \n",
    "    # Combine features\n",
    "    all_features = {**lexical_features, **descriptive_features}\n",
    "    \n",
    "    # Create a DataFrame for the features\n",
    "    features_df = pd.DataFrame([all_features])\n",
    "    \n",
    "    # Ensure the DataFrame matches the training data structure\n",
    "    # Add missing columns with default values\n",
    "    for col in df_final_columns:\n",
    "        if col not in features_df.columns:\n",
    "            features_df[col] = 0  # Or another appropriate default value\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    features_df = features_df[df_final_columns]\n",
    "    \n",
    "    # Drop columns that are not features (e.g., 'URL', 'Classification' if they were included)\n",
    "    features_to_drop = ['URL', 'Classification']  # Adjust based on your actual data\n",
    "    features_df = features_df.drop(columns=[col for col in features_to_drop if col in features_df.columns], errors='ignore')\n",
    "    \n",
    "    # Predict the classification\n",
    "    prediction = dTree.predict(features_df)\n",
    "    \n",
    "    return prediction[0]  # Assuming binary classification for simplicity\n",
    "\n",
    "# Use with caution if you are pasting in real malicious domains\n",
    "# CAUTION # url = \"c/l?u=10C78AC0&e=17AD89B&c=194D0D&t=0&email=WqS0CM9o%2BpbtiwumbI%2Fj2w%3D%3D&seq=1\"\n",
    "url = 'https://accounts.google.com/v3/'\n",
    "classification = predict_url_classification(url, dTree, df_final_columns)\n",
    "print(f\"Classes: 'benign': 0, 'defacement': 1, 'malware': 2, 'phishing': 3\")\n",
    "print(f\"Class of URL: {classification}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-class2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
